{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Программа работает с файлом \"hw_3_twitter.json\", расположенным в одной папке с программой. Файл считывается и построчно превращается в словари Python.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "twitter = []\n",
    "with open(\"hw_3_twitter.json\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        twitter.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункты 1-7 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 1 ###\n",
    "Поскольку каждая строка - это один твит, выясним, сколько твитов в наборе, посчитав длину получившегося в предыдущем действии списка (twitter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество твитов в наборе составляет 2556\n"
     ]
    }
   ],
   "source": [
    "twit_amount = len(twitter)\n",
    "print(\"Количество твитов в наборе составляет\", twit_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 2 ###\n",
    "Найдем процент удаленных твитов, посчитав количество твитов с пометкой \"delete\" и разделив его на общее количество твитов (значение округлено до сотых)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент удаленных твитов составляет 14.16 %\n"
     ]
    }
   ],
   "source": [
    "del_twit_amount = 0\n",
    "for tweet in twitter:\n",
    "    if \"delete\" in tweet:\n",
    "        del_twit_amount += 1\n",
    "if twit_amount:  # мы точно не знаем, что общее число твитов больше 0, поэтому защитимся от деления на 0\n",
    "    del_percentage = round((del_twit_amount / twit_amount) * 100, 2)\n",
    "    print(\"Процент удаленных твитов составляет\", del_percentage, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 3 ###\n",
    "Выясним, какие языки наиболее популярны. Для этого сначала составим список из всех языков, встретившихся в твитах (lang_list), при этом исключим из рассмотрения удаленные твиты, т.к. они не содержат информации о языке. Далее на основе полученного списка составим частотный словарь языков (freq_lang), отсортируем словарь по значениям в обратном порядке, добавим первые 10 ключей в список и \"красиво\" выведем нужную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 самых популярных языков: en, ja, es, ko, th, ar, und, in, pt, fr\n"
     ]
    }
   ],
   "source": [
    "lang_list = []\n",
    "for tweet in twitter:\n",
    "    if \"delete\" not in tweet:\n",
    "        lang = tweet[\"lang\"]\n",
    "        lang_list.append(lang)\n",
    "\n",
    "freq_lang = collections.Counter(lang_list)\n",
    "\n",
    "top10_langs = []\n",
    "for number, key in enumerate(sorted(freq_lang, key=freq_lang.get, reverse=True)):\n",
    "    if number < 10:\n",
    "        top10_langs.append(key)\n",
    "print(\"Топ-10 самых популярных языков:\", ', '.join(top10_langs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 4 ###\n",
    "Узнаем, есть ли твиты от одного и того же пользователя (**включая удаленные твиты**). Будем по одному добавлять id пользователей в список user_ids (для не-удаленных твитов id содержится внутри tweet[\"user\"][\"id\"], для удаленных - в поле tweet[\"delete\"][\"status\"][\"user_id\"]). Таким образом, в списке user_ids каждое имя пользователя будет встречаться столько раз, сколько твитов написал этот пользователь.\n",
    "Далее создадим частотный словарь с именами пользователей, добавим тех, кто написал более одного твита, в список repeated_users и выведем длину этого списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пользователей, написавших больше одного твита, составляет 66\n"
     ]
    }
   ],
   "source": [
    "user_ids = []\n",
    "for tweet in twitter:\n",
    "    if \"user\" in tweet:\n",
    "        user_id = tweet[\"user\"][\"id\"]\n",
    "    if \"delete\" in tweet:\n",
    "        user_id = tweet[\"delete\"][\"status\"][\"user_id\"]\n",
    "    user_ids.append(user_id)\n",
    "\n",
    "users = collections.Counter(user_ids)\n",
    "repeated_users = []\n",
    "for user in users:\n",
    "    if users[user] > 1:\n",
    "        repeated_users.append(user)\n",
    "print(\"Количество пользователей, написавших больше одного твита, составляет\", len(repeated_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 5 ###\n",
    "Выясним, какие хэштеги наиболее популярны (будем работать только с не-удаленными твитами, т.к. у удаленных тегов нет). Соберем все хэштеги, хранящиеся внутри [\"entities\"][\"hashtags\"], в общий список hashtags, и удалим из этого списка пустые элементы с помощью filter. Поскольку каждым элементом hashtags является не отдельный хэштег, а список хэштегов (+ дополнительная информация о них) из какого-либо твита, с помощью вложенного цикла вытащим собственно теги из [\"entities\"][\"hashtags\"][\"text\"] и сложим их в список cleared_hashtags. Далее создадим частотный словарь хэштегов, отсортируем его по убыванию значений и запишем первые 20 ключей в список top20_list, который затем \"красиво\" выведем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 хэштегов: BTS, 방탄소년단, AMAs, 人気投票ガチャ, 태형, 뷔, BTSinChicago, BTSLoveYourselfTour, 오늘의방탄, PledgeForSwachhBharat, MPN, PCAs, V, 시카고1회차공연, เป๊กผลิตโชค, JIMIN, running, NCT, 지민, WajahmuPlastik\n"
     ]
    }
   ],
   "source": [
    "hashtags = []\n",
    "for tweet in twitter:\n",
    "    if \"delete\" not in tweet:\n",
    "        hashtag_list = tweet[\"entities\"][\"hashtags\"]\n",
    "        hashtags.append(hashtag_list)\n",
    "\n",
    "hashtags = list(filter(None, hashtags))\n",
    "\n",
    "cleared_hashtags = []\n",
    "for element in hashtags:\n",
    "    for tags in element:\n",
    "        tag = tags[\"text\"]\n",
    "        cleared_hashtags.append(tag)\n",
    "\n",
    "top_hashtags = collections.Counter(cleared_hashtags)\n",
    "top20_list = []\n",
    "for hashtag in sorted(top_hashtags, key=top_hashtags.get, reverse=True):\n",
    "    top20_list.append(hashtag)\n",
    "print(\"Топ-20 хэштегов:\", \", \".join(top20_list[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 6 ###\n",
    "Составим частотный словарь всех слов в оригинальных твитах на английском языке. Сначала соберем все тексты подходящих (англоязычных, не-ретвитнутых и не-удаленных) твитов в список en_tweet_texts. Чтобы получить эти тексты, нужно использовать:\n",
    "\n",
    "1) поле \"extended_tweet\"[\"full_text\"], если оно есть (а есть оно не во всех твитах);\n",
    "\n",
    "2) поле \"text\" в остальных случаях.\n",
    "\n",
    "Далее пройдемся по каждому тексту в цикле и с помощью split разделим тексты на отдельные слова/числа/ссылки/эмодзи и т.д., после чего получившиеся единицы запишем в общий список full_tokens.\n",
    "Теперь обработаем получившиеся \"слова\". Приведем их к нижнему регистру с помощью lower, избавимся от эмодзи с помощью encode и decode, проверим, что слово не является ссылкой, хэштегом или обращением к пользователю (не начинается на http, # или @), после чего удалим знаки препинания, убедимся, что слово действительно является словом (а не, например, числом) и добавим его в общий список words. На основе полученного списка создадим частотный словарь freq_dict, отсортируем его по значениям в обратном порядке и выведем первые 10 пар \"ключ: значение\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 наиболее частотных слов:\n",
      "the - 134\n",
      "to - 96\n",
      "a - 74\n",
      "and - 69\n",
      "i - 68\n",
      "you - 54\n",
      "of - 51\n",
      "is - 50\n",
      "it - 49\n",
      "for - 44\n",
      "in - 43\n",
      "that - 35\n",
      "me - 31\n",
      "on - 30\n",
      "my - 27\n",
      "be - 25\n",
      "its - 24\n",
      "this - 22\n",
      "are - 22\n",
      "your - 21\n"
     ]
    }
   ],
   "source": [
    "en_tweet_texts = []\n",
    "for tweet in twitter:\n",
    "    if \"delete\" not in tweet and \"retweeted_status\" not in tweet and \"quoted_status\" not in tweet and tweet[\"lang\"] == \"en\":\n",
    "        if \"extended_tweet\" in tweet:\n",
    "            en_tweet_texts.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "        else:\n",
    "            en_tweet_texts.append(tweet[\"text\"])\n",
    "\n",
    "full_tokens = []\n",
    "for tweet in en_tweet_texts:\n",
    "    tokens = tweet.split()\n",
    "    full_tokens.extend(tokens)   \n",
    "\n",
    "words = []\n",
    "for token in full_tokens:\n",
    "    word = ((token.lower()).encode('ascii', 'ignore')).decode('ascii')\n",
    "    if not (word.startswith(\"http\") or word.startswith(\"#\") or word.startswith(\"@\")):\n",
    "        word = word.translate(word.maketrans(\"\", \"\", string.punctuation))\n",
    "        if word.isalpha():\n",
    "            words.append(word)\n",
    "\n",
    "print(\"Топ-20 наиболее частотных слов:\")\n",
    "freq_dict = collections.Counter(words)\n",
    "for number, word in enumerate(sorted(freq_dict, key=freq_dict.get, reverse=True)):\n",
    "    if number < 20:\n",
    "        print(word, \"-\", freq_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 7 ###\n",
    "Выясним, у кого из пользователей наибольшее число подписчиков. Будем работать только с не-удаленными твитами, поскольку в удаленных эта информация отсутствует.\n",
    "Пользователи могут быть либо авторами твитов, либо авторами ретвитов; проверим и то, и другое.\n",
    "Если твит является ретвитом, запишем в словарь с информацией о количестве подписчиков (followers_leaders) и автора самого твита, и автора ретвита, каждый раз проверяя, что данных этого пользователя еще нет в словаре. Если твит ретвитом не является, то просто запишем в словарь информацию о его авторе (также каждый раз проверяя наличие данных о нем).\n",
    "Отсортировав полученный словарь по значениям в обратном порядке, распечатаем первые 10 пар \"ключ: значение\", добавляя к данным подписи для \"красоты\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 пользователей с наибольшим количеством подписчиков:\n",
      "1 место - пользователь realDonaldTrump: 54808286 подписчиков\n",
      "2 место - пользователь jimmyfallon: 50469530 подписчиков\n",
      "3 место - пользователь nytimes: 42061723 подписчиков\n",
      "4 место - пользователь akshaykumar: 28150282 подписчиков\n",
      "5 место - пользователь NBA: 27340830 подписчиков\n",
      "6 место - пользователь BTS_twt: 16539919 подписчиков\n",
      "7 место - пользователь WSJ: 15991803 подписчиков\n",
      "8 место - пользователь pewdiepie: 15857270 подписчиков\n",
      "9 место - пользователь Forbes: 14928065 подписчиков\n",
      "10 место - пользователь Diddy: 14865069 подписчиков\n"
     ]
    }
   ],
   "source": [
    "followers_leaders = {}\n",
    "for tweet in twitter:\n",
    "    if \"delete\" not in tweet:\n",
    "        if \"retweeted_status\" in tweet:\n",
    "            retweet_follower_count = tweet[\"user\"][\"followers_count\"]\n",
    "            retweet_username = tweet[\"user\"][\"screen_name\"]\n",
    "            follower_count = tweet[\"retweeted_status\"][\"user\"][\"followers_count\"]\n",
    "            username = tweet[\"retweeted_status\"][\"user\"][\"screen_name\"]\n",
    "            if retweet_username not in followers_leaders:\n",
    "                followers_leaders[retweet_username] = retweet_follower_count\n",
    "        else:\n",
    "            follower_count = tweet[\"user\"][\"followers_count\"]\n",
    "            username = tweet[\"user\"][\"screen_name\"]\n",
    "        if username not in followers_leaders:\n",
    "            followers_leaders[username] = follower_count\n",
    "        \n",
    "print(\"Топ-10 пользователей с наибольшим количеством подписчиков:\")\n",
    "for number, key in enumerate(sorted(followers_leaders, key=followers_leaders.get, reverse=True)):\n",
    "    if number < 10:\n",
    "        string = str(number + 1) + \" место - пользователь \" + key + \": \" + str(followers_leaders[key]) + \" подписчиков\"\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 8 ##\n",
    "Выясним, каковы самые распространенные приложения для Твиттера. Вновь будем работать только с не-удаленными твитами.\n",
    "Как и в пункте 7, ситуация различается для оригинальных твитов и ретвитов: у оригинального твита только один источник, а у ретвита два - источник самого твита и источник ретвита. Учтем все случаи.\n",
    "Для оригинальных твитов извлечем источник из поля \"source\" при помощи регулярного выражения и запишем его в список sources. Для ретвитов найдем источник оригинального твита (поле \"retweeted_status\"[\"source\"]) и источник ретвита (поле \"source\") и запишем их в тот же список.\n",
    "Создадим частотный словарь источников твитов (sources_freq), отсортируем его по значениям в обратном порядке и распечатаем первые 10 элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 источников твита:\n",
      "'Twitter for iPhone'\n",
      "'Twitter for Android'\n",
      "'Twitter Web Client'\n",
      "'twittbot.net'\n",
      "'Twitter Lite'\n",
      "'TweetDeck'\n",
      "'Twitter for iPad'\n",
      "'IFTTT'\n",
      "'Facebook'\n",
      "'Hootsuite Inc.'\n"
     ]
    }
   ],
   "source": [
    "sources = []\n",
    "for tweet in twitter:\n",
    "    if \"delete\" not in tweet:\n",
    "        if \"retweeted_status\" not in tweet:\n",
    "            source = (re.search('<a href=\"(.*?)>(.*?)</a>', tweet[\"source\"])).group(2)\n",
    "        else:\n",
    "            source = (re.search('<a href=\"(.*?)>(.*?)</a>', tweet[\"retweeted_status\"][\"source\"])).group(2)\n",
    "            retweet_source = (re.search('<a href=\"(.*?)>(.*?)</a>', tweet[\"source\"])).group(2)\n",
    "            sources.append(retweet_source)\n",
    "        sources.append(source)\n",
    "\n",
    "sources_freq = collections.Counter(sources)\n",
    "print(\"Топ-10 источников твита:\")\n",
    "for number, source in enumerate(sorted(sources_freq, key=sources_freq.get, reverse=True)):\n",
    "    if number < 10:\n",
    "        pprint(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
